# Test Environment Setup

For initial setup of the test environment, see the Test Environment section of the
[development environment docs](https://github.com/catalogicsoftware/cloudcasa-charm/blob/master/docs/dev-env-setup.md#test-environement-setup).

This guide will discuss some additional testing considerations.

## Charm setup

Normally the CloudCasa charm would be deployed with the following commands:

```bash
juju add-model cloudcasa-system
juju deploy --trust cloudcasa
juju config cloudcasa clusterid=<Registered CloudCasa Cluster ID>
```

However, when deploying a locally build charm file, you must replace the ```juju deploy``` command with:

```bash
juju deploy --trust ./cloudcasa_ubuntu-20.04-amd64.charm --resource cloudcasa-image=catalogicsoftware/amds-kagent:latest 
```

The **Cluster ID** is a unique ID generated by CloudCasa when registering a cluster. 
The CloudCasa UI will provide the ID at the time of cluster registration.
You can also retrieve the ID for a cluster later by going to **Clusters/Overview** and selecting a cluster.
The ID will be displayed in parentheses right after cluster name on the cluster dashboard page.

You can check the installation progress with:

```bash
juju status
# or
watch -c juju status --color
```

You can view the debug log using the command:

```bash
juju debug-log
```

## Troubleshooting

### Kubelet fails to start with errors related to inotify_add_watch

For example, `systemctl status snap.kubelet.daemon.service` may report the following error:

```
kubelet.go:1414] "Failed to start cAdvisor" err="inotify_add_watch /sys/fs/cgroup/cpu,cpuacct: no space left on device"
```

This problem usually is related to the kernel parameters, `fs.inotify.max_user_instances` and `fs.inotify.max_user_watches`.

At first, you should increase their values on the machine that is hosting the test environment:

```bash
sysctl -w fs.inotify.max_user_instances=8192
sysctl -w fs.inotify.max_user_watches=1048576
```

Then, you can increase them also inside the worker containers:

```bash
juju config kubernetes-worker sysctl="{ fs.inotify.max_user_instances=8192 }"
juju config kubernetes-worker sysctl="{ fs.inotify.max_user_watches=1048576 }"
```

## Using Kind instead of MicroK8s

In some environments it may be desirable to use Kind instead of MicroK8s for testing.
You can setup a minimal configuration using the following procedure.
Testing with Kind using this procedure has not been fully verified.

First, create your cluster with `kind`:

```bash
kind create cluster --name charmed-kubernetes
```

Then, deploy the **juju OLM** into your local cluster:

```bash
juju add-k8s --client mycluster --cluster-name=kind-charmed-kubernetes
```

Start the operator lifecycle manager on your cluster:

```bash
juju bootstrap mycluster
```

Now your cluster has the **juju OLM** controller installed.

Now you can use `juju` to create **models** and **deploy** charms.

```bash
juju add-model cloudcasa-system
```

On Kubernetes, each model is put into a different namespace on the cluster. So you should see a `cloudcasa-system` namespace in your Kubernetes:

```bash
kubectl get namespaces
NAME                   STATUS   AGE
cloudcasa-system       Active   6s
controller-mycluster   Active   2m4s
default                Active   3m37s
kube-node-lease        Active   3m38s
kube-public            Active   3m38s
kube-system            Active   3m38s
local-path-storage     Active   3m33s
```

